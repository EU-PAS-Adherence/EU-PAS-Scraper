from datetime import datetime
import logging
import re

from eupas.commands import PandasCommand
from scrapy.exceptions import UsageError


class Command(PandasCommand):

    index_field = 'eu_pas_register_number'

    # NOTE: This field is generated by the patch command
    group_by_field_name = '$MATCHED_combined_centre_name'

    # NOTE: These fields are the subset of the analysed variables with multiple values delimited by '; '
    category_array_fields = ['age_population', 'data_source_types', 'funding_sources',
                             'non_interventional_scopes', 'non_interventional_study_design',
                             'special_population', 'study_topic']

    # NOTE: A datetime is required to determine the studies with past actual dates based on the day of extraction
    compare_datetime = None

    required_rmp = ['EU RMP category 1 (imposed as condition of marketing authorisation)',
                    'EU RMP category 2 (specific obligation of marketing authorisation)']

    # Maximal Excel sheet name length
    max_sheet_name_length = 31

    variables_seperator = '__'

    def syntax(self):
        return "[options]"

    def short_desc(self):
        return "Runs statistics with input file data."

    def add_options(self, parser):
        PandasCommand.add_options(self, parser)
        statistics = parser.add_argument_group(title="Custom Eupas Options")
        statistics.add_argument(
            "-D",
            "--date",
            metavar="COMPARE_DATE",
            default=None,
            help="specifies the date to compare against",
        )

    def process_options(self, args, opts):
        PandasCommand.process_options(self, args, opts)
        import numpy as np
        if opts.date:
            try:
                self.compare_datetime = np.datetime64(
                    opts.date, 'm')
            except ValueError:
                raise UsageError(
                    'The date was not formatted correctly like this')
        else:
            self.compare_datetime = np.datetime64(datetime.utcnow(), 'm')

    def preprocess(self, df):
        import numpy as np

        # NOTE: These fields have multiple values delimited by '; ' and will be split into an array
        array_fields = [
            # 'data_sources_types', 'other_documents_url' NOTE: Only for scraped
            # 'other_documents_name' NOTE: Only for exported
            'additional_institutions_encepp', 'age_population', 'countries',
            'data_sources_registered_with_encepp', 'funding_sources', 'medical_conditions', 'networks_encepp',
            'non_interventional_scopes', 'non_interventional_study_design',
            'references', 'special_population', 'study_topic',
            'substance_atc', 'substance_brand_name', 'substance_inn'
        ]

        yes_no_unknown_fields = [
            # 'conducted_data_characterisation' NOTE: Also uses 'Not applicable'
            'requested_by_regulator', 'check_conformance',
            'check_completeness', 'check_stability',
            'check_logical_consistency'
        ]

        # NOTE: Pandas reads boolean columns with NA Values as float
        # NOTE: We need to fill na first because NA will be True else
        df['$CANCELLED'] = df['$CANCELLED'].fillna(False).astype(bool)
        self.logger.info(
            f'Excluding {df["$CANCELLED"].astype(int).sum()} cancelled studies...'
        )
        df = df.loc[~df['$CANCELLED']]

        self.logger.info('Splitting array string fields into arrays')
        for field in array_fields:
            df[field] = df[field].str.split('; ')

        self.logger.info('Converting strings to bools')
        for field in yes_no_unknown_fields:
            df[field] = np.where(
                df[field] == 'Yes',
                True,
                np.where(
                    df[field] == 'No',
                    False,
                    self.pd.NA
                )
            )

        df = df.set_index(self.index_field).sort_index(axis='columns')

        return df

    def create_variables(self, df):
        import numpy as np

        # age_map = {
        #     'Preterm newborn infants (0 – 27 days)': '<18 years',
        #     'Term newborn infants (0 – 27 days)': '<18 years',
        #     'Infants and toddlers (28 days – 23 months)': '<18 years',
        #     'Children (2 to < 12 years)': '<18 years',
        #     'Adolescents (12 to < 18 years)': '<18 years',
        #     'Paediatric Population (< 18 years)': '<18 years',
        #     'Adults (18 to < 46 years)': '18-64 years',
        #     'Adults (46 to < 65 years)': '18-64 years',
        #     'Adults (65 to < 75 years)': '65+ years',
        #     'Adults (75 to < 85 years)': '65+ years',
        #     'Adults (85 years and over)': '65+ years',
        #     'Elderly (≥ 65 years)': '65+ years'
        # }

        age_map = {
            'Preterm newborn infants (0 – 27 days)': '<18 years',
            'Term newborn infants (0 – 27 days)': '<18 years',
            'Infants and toddlers (28 days – 23 months)': '<18 years',
            'Children (2 to < 12 years)': '<18 years',
            'Adolescents (12 to < 18 years)': '<18 years',
            'Paediatric Population (< 18 years)': '<18 years',
            'Adults (18 to < 46 years)': '18+ years',
            'Adults (46 to < 65 years)': '18+ years',
            'Adults (65 to < 75 years)': '18+ years',
            'Adults (75 to < 85 years)': '18+ years',
            'Adults (85 years and over)': '18+ years',
            'Elderly (≥ 65 years)': '18+ years'
        }

        variables = df.loc[:, [
            'study_type', 'state', 'risk_management_plan',
            'requested_by_regulator', 'number_of_subjects'
        ]]

        planned_duration = df.loc[
            df['final_report_date_planed'].notna()
            & df['data_collection_date_planed'].notna(),
            ['data_collection_date_planed', 'final_report_date_planed']] \
            .diff(axis='columns').iloc[:, -1]
        # NOTE: There are some studies with negative planned_duration
        planned_duration[planned_duration <= np.timedelta64(0)] = self.pd.NA

        def get_quartiles(s):
            result = (self.pd.qcut(s, 4, labels=False, duplicates='drop') + 1) \
                .fillna(0.0).astype(int)
            return np.where(result == 0, self.pd.NA, result)

        # 'data_sources_registered_with_encepp' => uses_established_data_source?
        # 'networks_encepp' => collaboration_with_research_network?

        variables = variables.assign(
            updated_state=df['$UPDATED_state'],
            registration_date=df['registration_date'].dt.year,
            number_of_countries=df['countries'].apply(len),
            number_of_countries_grouped=df['countries'].apply(
                lambda x: 'Single Country' if len(x) == 1 else 'Multiple Countries'),
            number_of_subjects_grouped=df['number_of_subjects'].apply(
                lambda x:
                '<100' if x < 100 else
                '100-<500' if x < 500 else
                '500-<1000' if x < 1000 else
                '1000-10000' if x < 10000 else
                '>10000'
            ),
            funding_sources=df['funding_sources'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; '),
            multiple_funding_sources=df['funding_sources'].apply(
                lambda x: len(x) if isinstance(x, list) else 0) > 1,
            age_population=df['age_population'].apply(
                lambda ages: '; '.join(sorted(list({age_map[x] for x in ages})))),
            special_population=df['special_population'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; '),
            study_topic=df['study_topic'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; '),
            non_interventional_scopes=df['non_interventional_scopes'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; '),
            non_interventional_study_design=df['non_interventional_study_design'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; '),
            has_medical_conditions=df['medical_conditions'].notna(),
            has_outcomes=df['outcomes'].notna(),
            planned_duration=planned_duration,
            planned_duration_quartiles=lambda x: get_quartiles(
                x['planned_duration'])
            # NOTE: Only scraped has data_source_types
            # data_source_types=df['data_source_types'].apply(
            #     lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; ')
        )

        return variables.sort_index(axis='columns')

    # def create_grouped_agg(self, df):
    #     import numpy as np

    #     dummy_fields = ['state', 'risk_management_plan']
    #     dummies = self.pd.get_dummies(df[dummy_fields]) \
    #         .rename(columns=self.python_name_converter)

    #     grouped = df.assign(
    #         past_data_collection=lambda x: x['data_collection_date_actual'].notna() &
    #         (x['data_collection_date_actual'] <= self.compare_datetime),
    #         past_data_collection_has_protocol=lambda x: x['past_data_collection'] & x['has_protocol'],
    #         past_final_report=lambda x: x['final_report_date_actual'].notna() &
    #         (x['final_report_date_actual'] <= self.compare_datetime),
    #         past_final_report_has_protocol=lambda x: x['past_final_report'] & x['has_result']
    #     ).merge(dummies, left_index=True, right_index=True).groupby(by=self.group_by_field_name, dropna=False)

    #     def set_sum(x: PandasCommand.pd.Series):
    #         return len(set(x.dropna().apply(list).sum()))

    #     def bool_sum(x: PandasCommand.pd.Series):
    #         return x.dropna().astype(float).sum()

    #     def setify(x: PandasCommand.pd.Series):
    #         return '; '.join(sorted(list(set(x.dropna().apply(list).sum()))))

    #     def mean_mean(x):
    #         return x.apply(np.mean).mean()

    #     dummie_agg = {
    #         f'number_of_studies_with_{col}': (col, bool_sum) for col in dummies
    #     }

    #     percentage_agg = {
    #         f'mean_{col}': (col, 'mean') for col in self.percentage_fields
    #     }

    #     grouped_agg = grouped.agg(
    #         number_of_collaborations_with_research_network=(
    #             'collaboration_with_research_network', bool_sum),
    #         number_of_countries=('countries', set_sum),
    #         set_of_countries=('countries', setify),
    #         number_of_studies_with_follow_up=('follow_up', bool_sum),
    #         min_number_of_subjects=('number_of_subjects', 'min'),
    #         max_number_of_subjects=('number_of_subjects', 'max'),
    #         mean_number_of_subjects=('number_of_subjects', 'mean'),
    #         median_number_of_subjects=('number_of_subjects', 'median'),
    #         number_of_studies_with_primary_outcomes=(
    #             'primary_outcomes', bool_sum),
    #         number_of_studies_with_secondary_outcomes=(
    #             'secondary_outcomes', bool_sum),
    #         number_of_studies_requested_by_regulator=(
    #             'requested_by_regulator', bool_sum),
    #         number_of_studies_using_established_data_sources=(
    #             'uses_established_data_source', bool_sum),
    #         **dummie_agg,
    #         **percentage_agg,
    #         mean_other_percentage=('funding_other_percentage', mean_mean),
    #         number_of_studies_with_result=('has_result', bool_sum),
    #         number_of_studies_with_protocol=('has_protocol', bool_sum),
    #         number_of_studies_with_past_data_collection=(
    #             'past_data_collection', bool_sum),
    #         number_of_studies_with_past_data_collection_and_protocol=(
    #             'past_data_collection_has_protocol', bool_sum),
    #         number_of_studies_with_final_report=(
    #             'past_final_report', bool_sum),
    #         number_of_studies_with_past_final_report_and_protocol=(
    #             'past_final_report_has_protocol', bool_sum)
    #     )

    #     sizes = grouped.size().rename('num_studies')
    #     grouped_agg = grouped_agg.merge(
    #         sizes,
    #         left_index=True,
    #         right_index=True
    #     )
    #     return grouped_agg

    def create_dummies(self, df, drop_references=True):

        # 'collaboration_with_research_network': False
        # 'uses_established_data_source': False
        dummy_without_na_drop_map = {
            'updated_state': 'Finalised',
            'study_type': 'Non-interventional study',
            'number_of_countries_grouped': 'Single Country',
            'multiple_funding_sources': False,
            'has_medical_conditions': True,
            # NOTE: Combined Categories => Many categories: binary encoding? or less categories by grouping
            # 'age_population': '18-64 years',
            'age_population': '18+ years',
            'number_of_subjects_grouped': '100-<500',
            'has_outcomes': False
        }

        # Add 'study_topic': 'Human medicinal product'?
        dummy_with_na_drop_map = {
            # NOTE: Combined Categories => Many categories: binary encoding?; Also one single NA field!
            # 'funding_sources': 'Pharmaceutical company and other private sector ',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'non_interventional_scopes': 'Assessment of risk minimisation measure implementation or effectiveness',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'non_interventional_study_design': 'Cohort',
            'planned_duration_quartiles': 1,
            'requested_by_regulator': False,
            'risk_management_plan': 'Not applicable',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'special_population': str(np.nan)
        }

        dummy_drop_map = {
            **dummy_without_na_drop_map,
            **dummy_with_na_drop_map
        }

        dummies = self.pd.concat([
            self.pd.get_dummies(
                df[dummy_without_na_drop_map.keys()],
                prefix_sep=self.variables_seperator,
                columns=dummy_without_na_drop_map.keys()
            ),
            self.pd.get_dummies(
                df[dummy_with_na_drop_map.keys()],
                prefix_sep=self.variables_seperator,
                columns=dummy_with_na_drop_map.keys(),
                dummy_na=True  # NOTE: This somehow renames int values to float values; For example 1 becomes 1.0
            ).rename(columns=lambda x: re.sub(r'\.0+$', '', x))  # NOTE: Quickfix for renaming; Important for reference-dropping
        ], axis='columns')

        if drop_references:
            columns_to_drop = [
                col for col in dummies
                if str(dummy_drop_map[col.split(self.variables_seperator)[0]]) == col.split(self.variables_seperator)[-1]
            ]

            dummies.drop(columns=columns_to_drop, inplace=True)

        return dummies.rename(columns=self.python_name_converter)

    # def create_binaries(self, df):
    #     binary_fields = [
    #         field
    #         for field in self.category_array_fields
    #         if field not in ['data_source_types', 'study_topic', 'funding_sources',
    #                          'non_interventional_scopes', 'non_interventional_study_design',
    #                          'special_population']
    #     ]

    #     def column_renamer(x, field):
    #         lowercase = re.sub(r'\s+', '_', x.lower())
    #         return f'{field}{self.variables_seperator}{lowercase}'

    #     return self.pd.concat([
    #         df[field].str.get_dummies('; ')
    #         .rename(columns=lambda x: column_renamer(x, field))
    #         for field in binary_fields
    #     ], axis='columns')

    def encode_variables(self, df, drop_references=True):
        dummies = self.create_dummies(df, drop_references)
        # binaries = self.create_binaries(df)
        # encoded = self.pd.concat([dummies, binaries], axis='columns') \
        #     .assign(registration_date=df['registration_date'])
        encoded = dummies.assign(registration_date=df['registration_date'])
        return encoded

    def run_logit(self, df, y, var_col_map):
        import statsmodels.formula.api as smf
        results = {}

        logging.captureWarnings(True)
        for var, cols in var_col_map.items():
            escaped_vars = [f'Q("{col}")' for col in cols]
            formula = f'{y} ~ {" + ".join(escaped_vars)}'
            self.logger.info(f'Running: {y} ~ {" + ".join(cols)}')
            lr_result = smf.logit(formula, df).fit(
                method='newton',
                maxiter=1000,
                warn_convergence=True,
                disp=False  # NOTE: Set to true/false to enable/disable printing convergence messages
            )
            results.setdefault(var, lr_result)
        logging.captureWarnings(False)

        return results

    def univariate_lr(self, df, y):

        variables = sorted({
            col.split(self.variables_seperator)[0] for col in df.columns
            if self.variables_seperator in col
        })
        var_col_map = {
            v: [col for col in df.columns if col.startswith(v)]
            for v in variables
        }
        var_col_map = {var: cols for var, cols in var_col_map.items() if cols}

        return self.run_logit(df, y, var_col_map)

    def multivariate_lr(self, df, y):
        high_corr_fiels = [
            # NOTE: This field should be single-valued (finalised) for studies with past final report date
            'updated_state',
            # NOTE: This field should only be true if the study is required by rmp?
            'requested_by_regulator',
            # NOTE: This field is only filled if the study is ongoing
            'planned_duration'
        ]

        drop_high_corr = [
            col for col in df.columns if col.split(self.variables_seperator)[0] in high_corr_fiels]

        var_col_map = {
            'all': df.drop([y, *drop_high_corr], axis='columns').columns
        }

        return self.run_logit(df, y, var_col_map)

    def run(self, args, opts):
        super().run(args, opts)
        import numpy as np
        import matplotlib as mpl
        import matplotlib.pyplot as plt
        from statsmodels.stats.proportion import proportion_confint

        self.logger = logging.getLogger()
        self.logger.info('Starting statistic script')
        self.logger.info(f'Pandas {self.pd.__version__}')
        self.logger.info('Reading input data...')
        data = self.preprocess(self.read_input())

        # Adding outcomes
        data = data.assign(
            has_protocol=data.filter(like='protocol').notna(),
            has_result=data.filter(like='result').notna().any(axis='columns')
        )

        self.logger.info('Generating categories...')
        variables = self.create_variables(data)
        variables = variables.merge(
            data.loc[:, [
                'data_collection_date_actual', 'final_report_date_actual',
                'has_protocol', 'has_result'
            ]],
            left_index=True,
            right_index=True
        )

        self.logger.info('Writing some preanalysis data...')
        self.write_output(data, '_statistics_preprocessed')
        self.write_output(variables, '_statistics_variables')

        variables_past_data_collection = variables[variables['data_collection_date_actual'].notna(
        ) & (variables['data_collection_date_actual'] <= self.compare_datetime)]

        variables_two_weeks_past_final_report = variables[variables['final_report_date_actual'].notna(
        ) & (variables['final_report_date_actual'] <= self.compare_datetime - np.timedelta64(14, 'D'))]

        variables_past_data_collection_without_two_weeks_past_final_report = \
            variables_past_data_collection[
                ~variables_past_data_collection.index.isin(
                    variables_two_weeks_past_final_report.index)]

        self.logger.info('Generating and writing part 1 of analysis...')
        for df, suffix in [
                (variables, '_all'),
                (variables_past_data_collection,
                 '_past_date_collection'),
                (variables_two_weeks_past_final_report,
                 '_two_weeks_past_final_report'),
                (variables_past_data_collection_without_two_weeks_past_final_report,
                 '_past_data_collection_without_two_weeks_past_final_report')]:

            with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_variables_frequencies{suffix}.xlsx', engine='openpyxl') as writer:

                # Variables
                df.to_excel(
                    writer,
                    sheet_name=f'variables{suffix}'[
                        :self.max_sheet_name_length]
                )

                # Description of all numerical fields
                # min max mean var etc.
                df.describe().to_excel(
                    writer,
                    sheet_name='numerical_descriptions'
                )

                for col in sorted(df.columns):

                    def calculate_and_write_frequencies(df, col, writer, dropna=True, col_offset=0):

                        # Absolute and relative frequencies of categories
                        frequencies = self.pd.DataFrame().assign(
                            absolute=df.loc[:, [col]].apply(
                                lambda x: x.value_counts(dropna=dropna)),
                            percentage=df.loc[:, [col]].apply(
                                lambda x: x.value_counts(dropna=dropna, normalize=True) * 100)
                        ).reset_index()

                        frequencies.to_excel(
                            writer,
                            sheet_name=f'{col}_frequencies'[
                                :self.max_sheet_name_length],
                            index=False,
                            startcol=col_offset
                        )

                        if col in self.category_array_fields:

                            grouped_frequencies = frequencies \
                                .assign(split=lambda x: x[col].str.split('; ')) \
                                .explode('split') \
                                .groupby('split')

                            # Absolute and relative frequencies of subcategories
                            overall_frequencies = self.pd.DataFrame().assign(
                                overall_absolute=grouped_frequencies['absolute'].sum(
                                ),
                                overall_percentage=grouped_frequencies['percentage'].sum(
                                ),
                            ).reset_index().rename(columns={'split': col})

                            overall_frequencies.to_excel(
                                writer,
                                sheet_name=f'{col}_frequencies'[
                                    :self.max_sheet_name_length],
                                index=False,
                                startcol=4 + col_offset
                            )

                    calculate_and_write_frequencies(df, col, writer)
                    if df.loc[:, [col]].isna().any().loc[col]:
                        calculate_and_write_frequencies(
                            df, col, writer, dropna=False,
                            col_offset=8 if col in self.category_array_fields else 4
                        )

        self.logger.info('Generating and writing part 2 of analysis...')
        for df, suffix in [
                (variables, '_all'),
                (variables_past_data_collection,
                 '_past_date_collection'),
                (variables_two_weeks_past_final_report,
                 '_two_weeks_past_final_report')]:

            with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_variables_documents{suffix}.xlsx', engine='openpyxl') as writer:

                df.to_excel(
                    writer,
                    sheet_name=f'variables{suffix}'[
                        :self.max_sheet_name_length]
                )

                for col in ['has_protocol', 'has_result']:

                    def get_frequencies_with_ci(df, alpha=0.05):
                        return df.apply(lambda x: x.value_counts()) \
                            .rename(columns={col: 'absolute'}) \
                            .assign(
                                percentage=df.apply(
                                    lambda x: x.value_counts(normalize=True) * 100),
                                confidence_interval=lambda x: x['absolute'].apply(
                                    lambda y: [z * 100 for z in proportion_confint(y, len(df), alpha=0.05, method='beta')])
                        ).reset_index()

                    # Absolute and relative frequencies (with 95%-CI) of categories with protocols or results
                    frequencies = get_frequencies_with_ci(df.loc[:, [col]])

                    frequencies.to_excel(
                        writer,
                        sheet_name=col[:self.max_sheet_name_length],
                        index=False
                    )

                    # Same metrics for the subset of studies required by RMP
                    required_rmp_frequencies = get_frequencies_with_ci(
                        df.loc[df['risk_management_plan'].isin(
                            self.required_rmp), [col]]
                    ).rename(columns={col: f'required_{col}'})

                    required_rmp_frequencies.to_excel(
                        writer,
                        sheet_name=col[:self.max_sheet_name_length],
                        index=False,
                        startrow=4
                    )

        # self.logger.info('Generating and writing part 3 of analysis...')
        # data_to_group = variables.merge(
        #     data.loc[:, [self.group_by_field_name, *self.percentage_fields,
        #                  'funding_other_percentage', 'countries']],
        #     left_index=True,
        #     right_index=True
        # )
        # grouped_agg = self.create_grouped_agg(data_to_group)
        # self.write_output(grouped_agg, '_statistics_centre_all')

        self.logger.info(
            'Generating and writing correlation matrix for logistic regression...')
        y = variables.loc[:, ['has_protocol', 'has_result']].astype(int)
        encoded_all = self.encode_variables(
            variables, drop_references=False)
        encoded_all_y = encoded_all.merge(
            y,
            left_index=True,
            right_index=True,
            how='right'
        )
        correlations_all = encoded_all_y.corr()
        self.write_output(
            correlations_all, '_statistics_encoded_variables_correlations_all')

        for df, y_label, name in [
                (variables_past_data_collection, 'has_protocol', 'protocol'),
                (variables_two_weeks_past_final_report, 'has_result', 'results')]:

            self.logger.info(
                f'Starting univariate logistic regression for {y_label}...')
            self.logger.info(
                'Generating and writing encoded variables for logistic regression...')
            encoded = self.encode_variables(df)
            y = df.loc[:, [y_label]].astype(int)
            encoded_y = encoded.merge(
                y,
                left_index=True,
                right_index=True,
                how='right'
            )
            self.write_output(
                encoded_y, f'_statistics_encoded_variables_{name}')

            self.logger.info(
                'Generating and writing correlations for logistic regression...')
            encoded_with_references = self.encode_variables(
                df, drop_references=False)
            encoded_with_references_y = encoded_with_references.merge(
                y,
                left_index=True,
                right_index=True,
                how='right'
            )

            correlations_y = encoded_with_references_y \
                .corr(method='pearson')[y_label] \
                .drop(y_label) \
                .rename(f'{y_label}_pearson_correlation_coefficient')
            self.write_output(
                correlations_y.to_frame(), f'_statistics_encoded_variables_correlations_for_{name}_model')

            def save_model_results(results, folder_name, subfolder_name):
                (self.output_folder / folder_name / 'models' /
                 subfolder_name).mkdir(parents=True, exist_ok=True)
                (self.output_folder / folder_name / 'summaries' /
                 subfolder_name).mkdir(parents=True, exist_ok=True)

                for file_name, model_result in results.items():
                    model_result.save(
                        self.output_folder / folder_name / 'models' / subfolder_name / f'{file_name}.pickle')

                    (self.output_folder / folder_name / 'summaries' / subfolder_name / f'{file_name}.txt') \
                        .write_text(model_result.summary().as_text())

                    (self.output_folder / folder_name / 'summaries' / subfolder_name / f'{file_name}.html') \
                        .write_text(model_result.summary().as_html())

                    (self.output_folder / folder_name / 'summaries' / subfolder_name / f'{file_name}.csv') \
                        .write_text(model_result.summary().as_csv())

            self.logger.info(
                'Running univariate logistic regression and writing output...')
            results = self.univariate_lr(encoded_y, y_label)
            save_model_results(results, 'univariate_models', name)

            self.logger.info(
                'Running multivariate logistic regression and writing output...')
            results = self.multivariate_lr(encoded_y, y_label)
            save_model_results(results, 'multivariate_models', name)

        self.logger.info('Generating and writing plots...')
        (self.output_folder / 'plots/').mkdir(parents=True, exist_ok=True)
        mpl.style.use('bmh')

        import seaborn as sns
        sns.set_theme(context="paper", style="whitegrid")
        fig, ax = plt.subplots(figsize=(20, 15), dpi=300)
        ax.set_title('Correlations (Pearson)')
        mask = np.triu(np.ones_like(correlations_all, dtype=bool))
        sns.heatmap(correlations_all, mask=mask, cmap='RdBu', vmax=.3, center=0,
                    square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
        fig.savefig(self.output_folder / 'plots' /
                    'correlation_heatmap.png', bbox_inches='tight')

        plt.figure(dpi=300)
        date = data['registration_date'].dt.to_period('M')
        self.pd.concat(
            [
                data.groupby(date).size().rename('studies'),
                data.groupby(date).size().cumsum().rename('cumulated studies')
            ], axis='columns') \
            .plot(
                title='Frequency of studies by "Registration Date"',
                xlabel='Registration Date',
                ylabel='# of studies',
                subplots=True
        )
        plt.savefig(self.output_folder / 'plots' /
                    'registration_date_count_freq.png')

        plt.figure(dpi=300)
        variables.groupby(date)[['has_protocol', 'has_result']].sum().plot(
            title='Frequency of studies with protocol or results by "Registration Date"',
            xlabel='Registration Date',
            ylabel='# of studies',
            subplots=True
        )
        plt.savefig(self.output_folder / 'plots' /
                    'registration_date_protocol_results_freq.png')

        for col in ['number_of_countries', 'number_of_subjects']:
            plt.figure(figsize=(5, 10), dpi=300)
            variables[col].plot(
                kind='box', title=f'Boxplot of study categories by "{self.excel_name_converter(col)}"',)
            # sns.violinplot(
            #     data=variables[col], bw_adjust=.5, cut=1, linewidth=1, palette="Set3")
            plt.savefig(self.output_folder / 'plots' / f'{col}_boxplot.png')

        plt.figure(figsize=(15, 3), dpi=300)
        variables['planned_duration'].map(lambda x: x.days).plot(
            kind='kde', title='Density of "Planned Duration"')
        plt.savefig(self.output_folder / 'plots' /
                    'planned_duration_density.png')
