from datetime import datetime, timezone
import logging
import re

from eupas.commands import PandasCommand
from scrapy.exceptions import UsageError


class Command(PandasCommand):

    ################################
    #     IMPORTANT FIELD NAMES    #
    ################################
    index_field = 'eu_pas_register_number'

    # NOTE: This fields are generated by the patch command
    cancel_field = '$CANCELLED_MANUAL'
    group_by_field_name = '$MATCHED'

    ################################
    #             DATE             #
    ################################
    # NOTE: A datetime is required to determine the studies with past actual dates based on the day of extraction
    compare_datetime = None
    protocol_tolerance_busdays = 5
    results_tolerance_busdays = 10 + 5
    # NOTE: Encepp migrated from EUPAS go EMA RWD and has reported a downtime between 2024-01-22 and 2024-02-15
    # NOTE: There are studies with update_dates 2024-01-22 and 2024-02-15
    downtime_start = '2024-01-23'
    downtime_end = '2024-02-14'

    ################################
    #  LISTS USED IN RUN FUNCTION  #
    ################################
    required_rmp = ['EU RMP category 1 (imposed as condition of marketing authorisation)',
                    'EU RMP category 2 (specific obligation of marketing authorisation)']

    # NOTE: These fields are a subset of the analysed variables containing multiple values delimited by '; '
    category_array_fields = ['age_population', 'data_source_types', 'funding_sources',
                             'non_interventional_scopes', 'non_interventional_study_design',
                             'special_population', 'study_topic']

    # Used for frequency tables
    # NOTE: Arrangement will be mirrored in the final table
    frequency_fields = [
        'age_population',
        'collaboration_with_research_network',
        'funding_sources_grouped',
        'studied_medical_conditions',
        'has_outcomes',
        'multiple_funding_sources',
        'number_of_countries_grouped',
        'number_of_subjects_grouped',
        'planned_duration_quartiles',
        'registration_year_grouped',
        'requested_by_regulator',
        # 'study_topic_grouped', # Highly correlated new field, not useful at time of extraction
        'study_type',
        'updated_state',
        'uses_established_data_source',
        'has_protocol',
        'has_result'
    ]

    ################################
    #            OTHER             #
    ################################
    # Maximal Excel sheet name length
    max_sheet_name_length = 31

    # Dummy encoded variables use this seperator e.g updated_state__planned, updated_state__ongoing, updated_state__finalised
    variables_seperator = '__'

    # Regex to unescape variable name in patsy formula string
    formula_formatter_regex = re.compile(r'Q\("(.*?)"\).*')

    def syntax(self):
        return "[options]"

    def short_desc(self):
        return "Runs statistics with input file data."

    def add_options(self, parser):
        PandasCommand.add_options(self, parser)
        statistics = parser.add_argument_group(title="Custom Eupas Options")
        statistics.add_argument(
            "-D",
            "--date",
            metavar="COMPARE_DATE",
            default=None,
            help="specifies a date to compare against",
        )

    def process_options(self, args, opts):
        PandasCommand.process_options(self, args, opts)
        import numpy as np
        if opts.date:
            try:
                self.compare_datetime = np.datetime64(
                    opts.date, 'm'
                )
            except ValueError:
                raise UsageError(
                    'The date was not formatted correctly like e.g 2020-12-31T15:45'
                )
        else:
            self.compare_datetime = np.datetime64(
                datetime.now(timezone.utc), 'm'
            )

    def preprocess(self, df):
        '''
        Excludes cancelled studies and applies useful transformations.
        '''
        import numpy as np

        ###################################
        #    EXCLUDE CANCELLED STUDIES    #
        ###################################
        # NOTE: Pandas reads boolean columns with NA Values as float
        # NOTE: We need to fill na first because NA will be True else
        df[self.cancel_field] = df[self.cancel_field] \
            .fillna(False).astype(bool)
        self.logger.info(
            f'Excluding {df[self.cancel_field].astype(int).sum()} cancelled studies...'
        )
        df = df.loc[~df[self.cancel_field]]

        ###################################
        # SPLIT VALUES JOINED BY SPERATOR #
        ###################################
        # NOTE: These fields have multiple values delimited by '; ' and will be split into an array
        array_fields = [
            # 'data_sources_types', 'other_documents_url' NOTE: Only for scraped
            # 'other_documents_name' NOTE: Only for exported
            'additional_institutions_encepp', 'age_population', 'countries',
            'data_sources_registered_with_encepp', 'funding_sources', 'medical_conditions', 'networks_encepp',
            'non_interventional_scopes', 'non_interventional_study_design',
            'references', 'special_population', 'study_topic',
            'substance_atc', 'substance_brand_name', 'substance_inn'
        ]

        self.logger.info('Splitting array string fields into arrays')
        for field in array_fields:
            df[field] = df[field].str.split('; ')

        ###################################
        #      CONVERT YES/NO TO BOOL     #
        ###################################
        yes_no_unknown_fields = [
            # 'conducted_data_characterisation' NOTE: Also uses 'Not applicable'
            'requested_by_regulator', 'check_conformance',
            'check_completeness', 'check_stability',
            'check_logical_consistency'
        ]

        self.logger.info('Converting strings to bools')
        for field in yes_no_unknown_fields:
            df[field] = np.where(
                df[field] == 'Yes',
                True,
                np.where(
                    df[field] == 'No',
                    False,
                    self.pd.NA
                )
            )

        ###################################
        #        REINDEX AND SORT         #
        ###################################
        df = df.set_index(self.index_field).sort_index(axis='columns')

        ###################################
        #   CORRECT IMPLAUSIBLE OUTLIERS  #
        ###################################
        corrected_date_38752 = np.datetime64(
            datetime(
                year=2020,
                month=12,
                day=22
            ), 'm'
        )
        df.loc[38752, 'data_collection_date_planed'] = corrected_date_38752
        df.loc[38752, 'data_collection_date_actual'] = corrected_date_38752

        # corrected_number_of_subjects_map = {
        #     # Information from research paper: https://doi.org/10.1016/j.vaccine.2015.05.013
        #     8429: 99_999_999, # Metastudy (39 studies) with multimillion sized populations in each study
        #     # Information from research paper: https://doi.org/10.1007/s13340-018-0360-4
        #     16082: 3_499_600, # No consequences (would still be grouped as >10000)
        #     # Information from study results: https://catalogues.ema.europa.eu/node/1906/methodological-aspects
        #     25192: 99_999_999, # Population of Europe (EudraVigilance and VigiBase)
        #     # Information from study results: https://catalogues.ema.europa.eu/node/1985/methodological-aspects
        #     27963: 99_999_999 # Population of Europe (EudraVigilance)
        # }
        # for key, value in corrected_number_of_subjects_map.items():
        #     df.loc[key, 'number_of_subjects'] = value

        return df

    def create_variables(self, df):
        '''
        Creates mostly categorical variables from preprocessed Dataframe.
        '''
        import numpy as np

        # HELPER MAPS AND FUNCTIONS

        age_map = {
            'Preterm newborn infants (0 – 27 days)': '<18 years',
            'Term newborn infants (0 – 27 days)': '<18 years',
            'Infants and toddlers (28 days – 23 months)': '<18 years',
            'Children (2 to < 12 years)': '<18 years',
            'Adolescents (12 to < 18 years)': '<18 years',
            'Paediatric Population (< 18 years)': '<18 years',
            'Adults (18 to < 46 years)': '18+ years',
            'Adults (46 to < 65 years)': '18+ years',
            'Adults (65 to < 75 years)': '18+ years',
            'Adults (75 to < 85 years)': '18+ years',
            'Adults (85 years and over)': '18+ years',
            'Elderly (≥ 65 years)': '18+ years'
        }

        funding_map = {
            'EMA': 'Public',
            'EU institutional research programme': 'Public',
            'No external funding': self.pd.NA,
            'Non for-profit organisation (e.g. charity)': 'Public',
            'Other': 'Other',
            'Pharmaceutical company and other private sector ': 'Private'
        }

        def get_and_save_quartiles(s):
            # Get quartiles
            quartiles, bins = self.pd.qcut(
                s, 4, labels=False, retbins=True, duplicates='drop')
            quartiles = (quartiles + 1).fillna(0.0).astype(int)
            quartiles = np.where(quartiles == 0, self.pd.NA, quartiles)

            # Save quartile intervals
            self.logger.info(f'Quartile Intervals for {s.name}:\n{bins}')
            np.save(
                self.output_folder / f'{s.name}_quartile_intervals.npy',
                bins
            )

            return quartiles

        # DURATION VARIABLE

        planned_duration = df.loc[
            df['final_report_date_planed'].notna()
            & df['data_collection_date_planed'].notna(),
            ['data_collection_date_planed', 'final_report_date_planed']] \
            .diff(axis='columns').iloc[:, -1]
        # NOTE: There are some studies with negative planned_duration
        planned_duration[planned_duration <= np.timedelta64(0)] = self.pd.NA

        # COPY UNCHANGED VARIABLES

        variables = df.loc[:, [
            'study_type', 'state', 'risk_management_plan',
            'requested_by_regulator', 'number_of_subjects',
            'registration_date',
            # HELPER VARIABLES
            'data_collection_date_actual', 'final_report_date_actual',
            'has_protocol', 'has_result'
        ]]

        # ASSIGN OTHER VARIABLES

        variables = variables.assign(
            updated_state=df['$UPDATED_state'],
            registration_year=df['registration_date'].dt.year,
            registration_year_grouped=lambda x: x['registration_year'].apply(
                lambda y:
                '2010-2013' if y <= 2013 else
                '2023-2024' if y >= 2023 else
                str(y)
            ),
            number_of_countries=df['countries'].apply(len),
            number_of_countries_grouped=df['countries'].apply(
                lambda x: 'Single Country' if len(x) == 1 else 'Multiple Countries'),
            number_of_subjects_grouped=df['number_of_subjects'].apply(
                lambda x:
                '<100' if x < 100 else
                '100-<500' if x < 500 else
                '500-<1000' if x < 1000 else
                '1000-10000' if x < 10000 else
                '>10000'
            ),
            funding_sources=df['funding_sources'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x
            ).str.join('; '),
            funding_sources_grouped=df['funding_sources'].apply(
                lambda sources:
                'Other' if not isinstance(sources, list) else
                'Other' if {'No external funding'}.issuperset(sources) else
                '; '.join(sorted(list(
                    {'Public', 'Private'} & set(
                        funding_map[x] for x in sources)
                ))) or 'Other'
            ),
            multiple_funding_sources=df['funding_sources'].apply(
                lambda x: len(x) if isinstance(x, list) else 0
            ) > 1,
            age_population=df['age_population'].apply(
                lambda ages: '; '.join(
                    sorted(list({age_map[x] for x in ages}))
                )
            ),
            special_population=df['special_population'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x
            ).str.join('; '),
            study_topic=df['study_topic'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x
            ).str.join('; '),
            study_topic_grouped=df['study_topic'].apply(
                lambda topics:
                self.pd.NA if not isinstance(topics, list) else
                '; '.join(sorted(list(
                    {'Disease /health condition',
                        'Human medicinal product'} & set(topics)
                ))) or 'Other'
            ),
            non_interventional_scopes=df['non_interventional_scopes'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x
            ).str.join('; '),
            non_interventional_study_design=df['non_interventional_study_design'].apply(
                lambda x: list(sorted(x)) if isinstance(x, list) else x
            ).str.join('; '),
            studied_medical_conditions=df['medical_conditions'].notna(),
            has_outcomes=df['outcomes'].notna(),
            planned_duration=planned_duration,
            planned_duration_quartiles=lambda x: get_and_save_quartiles(
                x['planned_duration']
            ),
            uses_established_data_source=df[[
                'data_sources_registered_with_encepp',
                'data_sources_not_registered_with_encepp'
            ]].notna().any(axis='columns'),
            collaboration_with_research_network=df[
                ['networks_encepp', 'networks_not_encepp']
            ].notna().any(axis='columns'),
            # NOTE: Only scraped has data_source_types
            # data_source_types=df['data_source_types'].apply(
            #     lambda x: list(sorted(x)) if isinstance(x, list) else x).str.join('; ')

            # HELPER VARIABLES
            data_collection_days_difference=np.busday_count(
                df['data_collection_date_actual']
                .fillna(self.compare_datetime + np.timedelta64(1, 'D')).dt.date.tolist(),
                np.datetime64(self.compare_datetime, 'D'),
                weekmask='1111111',
                holidays=self.downtime
            ),
            data_collection_busdays_difference=np.busday_count(
                df['data_collection_date_actual']
                .fillna(self.compare_datetime + np.timedelta64(1, 'D')).dt.date.tolist(),
                np.datetime64(self.compare_datetime, 'D'),
                holidays=self.downtime
            ),
            due_protocol=lambda x: x['data_collection_busdays_difference'] > self.protocol_tolerance_busdays,
            final_report_days_difference=np.busday_count(
                df['final_report_date_actual']
                .fillna(self.compare_datetime + np.timedelta64(1, 'D')).dt.date.tolist(),
                np.datetime64(self.compare_datetime, 'D'),
                weekmask='1111111',
                holidays=self.downtime
            ),
            final_report_busdays_difference=np.busday_count(
                df['final_report_date_actual']
                .fillna(self.compare_datetime + np.timedelta64(1, 'D')).dt.date.tolist(),
                np.datetime64(self.compare_datetime, 'D'),
                holidays=self.downtime
            ),
            due_result=lambda x: x['final_report_busdays_difference'] > self.results_tolerance_busdays,
        )

        return variables.sort_index(axis='columns')

    def create_grouped_agg(self, df):
        '''
        Groups studies by sponsors and creates aggregated statistics for each group.

        Uses categorical Variables in Dataframe as input.
        '''
        # NOTE: Can't access DataFrameGroupBy.groups with NA Value
        temp_na_name = '$NA'

        # CREATE DUMMIE COLUMNS FOR EASY AGGREGATIONS

        dummy_fields = ['updated_state', 'risk_management_plan']
        dummies = self.pd.get_dummies(df[dummy_fields], dummy_na=True) \
            .rename(columns=self.python_name_converter)

        # ASSIGN VARIABLES FOR STATISTICS OF REPORTED DOCUMENTS

        grouped = df.assign(
            due_protocol_has_protocol=lambda x:
                x['due_protocol'] & x['has_protocol'],
            due_result_has_result=lambda x:
                x['due_result'] & x['has_result']
        ).merge(dummies, left_index=True, right_index=True)

        # GROUPBY SPONSORS AFTER EXPLODING THE STUDIES WITH MULTIPLE SPONSORS

        grouped.loc[:, self.group_by_field_name] =  \
            grouped.loc[:, self.group_by_field_name].str.split('; ') \
            .fillna(temp_na_name)

        grouped = grouped.explode(self.group_by_field_name) \
            .groupby(by=self.group_by_field_name, dropna=False)

        # HELPER FUNCTIONS AND VARIABLE

        def set_sum(x):
            return len(set(x.dropna().apply(list).sum()))

        def bool_sum(x):
            return x.dropna().astype(float).sum()

        def setify(x):
            return '; '.join(sorted(list(set(x.dropna().apply(list).sum()))))

        dummie_agg = {
            f'number_of_studies_with_{col}': (col, bool_sum) for col in dummies
        }

        # AGGREGATE STATISTICS

        grouped_agg = grouped.agg(
            number_of_countries=('countries', set_sum),
            set_of_countries=('countries', setify),
            # number_of_collaborations_with_research_network=('collaboration_with_research_network', bool_sum),
            # min_number_of_subjects=('number_of_subjects', 'min'),
            # max_number_of_subjects=('number_of_subjects', 'max'),
            # mean_number_of_subjects=('number_of_subjects', 'mean'),
            # median_number_of_subjects=('number_of_subjects', 'median'),
            # number_of_studies_with_outcomes=('has_outcomes', bool_sum),
            # number_of_studies_requested_by_regulator=('requested_by_regulator', bool_sum),
            # number_of_studies_using_established_data_sources=('uses_established_data_source', bool_sum),
            **dummie_agg,
            number_of_studies_with_result=('has_result', bool_sum),
            number_of_studies_with_protocol=('has_protocol', bool_sum),
            number_of_studies_with_due_protocol=('due_protocol', bool_sum),
            number_of_studies_with_due_protocol_has_protocol=(
                'due_protocol_has_protocol', bool_sum
            ),
            number_of_studies_with_due_result=('due_result', bool_sum),
            number_of_studies_with_due_result_has_result=(
                'due_result_has_result', bool_sum
            )
        )

        sizes = grouped.size().rename('num_studies')
        grouped_agg = grouped_agg.merge(
            sizes,
            left_index=True,
            right_index=True
        )

        ids = self.pd.Series({
            k: '; '.join(str(s) for s in sorted(v))
            for [k, v] in grouped.groups.items()
        }).rename('study_ids')
        grouped_agg = grouped_agg.merge(
            ids,
            left_index=True,
            right_index=True
        )

        return grouped_agg.rename(index={temp_na_name: self.pd.NA})

    def encode_variables(self, df, drop_references=True):
        '''
        Creates encoded variables for logistic regression.
        '''
        import numpy as np

        # DEFINE REFERENCE VALUES FOR VARIABLES WITH OR WITHOUT NA VALUES
        # NOTE: only used to drop references if specified

        dummy_without_na_drop_map = {
            'updated_state': 'Finalised',
            'study_type': 'Non-interventional study',
            'number_of_countries_grouped': 'Single Country',
            'funding_sources_grouped': 'Private',
            'multiple_funding_sources': False,
            'age_population': '18+ years',
            'number_of_subjects_grouped': '100-<500',
            'studied_medical_conditions': True,
            'has_outcomes': False,
            'collaboration_with_research_network': False,
            'uses_established_data_source': False,
            'registration_year_grouped': '2010-2013'
        }

        dummy_with_na_drop_map = {
            'planned_duration_quartiles': 1,
            'requested_by_regulator': False,
            'risk_management_plan': 'Not applicable',
            'study_topic_grouped': str(np.nan),
            # NOTE: Combined Categories => Many categories: binary encoding?; Also one single NA field!
            # 'funding_sources': 'Pharmaceutical company and other private sector ',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'non_interventional_scopes': 'Assessment of risk minimisation measure implementation or effectiveness',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'non_interventional_study_design': 'Cohort',
            # NOTE: Combined Categories => Many categories: binary encoding?
            # 'special_population': str(np.nan)
        }

        # ONE-HOT ENCODE VARIABLES

        encoded = self.pd.concat([
            self.pd.get_dummies(
                df[dummy_without_na_drop_map.keys()],
                prefix_sep=self.variables_seperator,
                columns=dummy_without_na_drop_map.keys()
            ),
            self.pd.get_dummies(
                df[dummy_with_na_drop_map.keys()],
                prefix_sep=self.variables_seperator,
                columns=dummy_with_na_drop_map.keys(),
                dummy_na=True  # NOTE: This somehow renames int values to float values; For example 1 becomes 1.0
            ).rename(columns=lambda x: re.sub(r'\.0+$', '', x))  # NOTE: Quickfix for renaming; Important for reference-dropping
        ], axis='columns')

        # DROP REFERENCES IF SPECIFIED

        if drop_references:
            dummy_drop_map = {
                **dummy_without_na_drop_map,
                **dummy_with_na_drop_map
            }

            columns_to_drop = [
                col for col in encoded
                if str(dummy_drop_map[col.split(self.variables_seperator)[0]]) == col.split(self.variables_seperator)[-1]
            ]

            encoded.drop(columns=columns_to_drop, inplace=True)

        # CONTINUOUS VARIABLE

        encoded = self.pd.concat([
            encoded,
            df[[
                'data_collection_days_difference',
                'final_report_days_difference'
            ]]
        ], axis='columns')

        return encoded.rename(columns=self.python_name_converter)

    def run_logit(self, df, logit_map):
        '''
        Runs logistic regression with patsy formulas. Uses a {name: formula} as input and {name: logit_results} as output.
        '''
        import statsmodels.formula.api as smf
        results = {}

        logging.captureWarnings(True)
        for name, formula, info in logit_map:
            self.logger.info(
                f'Running: {info}')
            lr_result = smf.logit(formula, df).fit(
                method='newton',
                maxiter=1000,
                warn_convergence=True,
                disp=False  # NOTE: Set to true/false to enable/disable printing convergence messages
            )
            results.setdefault(name, lr_result)
        logging.captureWarnings(False)

        return results

    def build_formula_string(self, y, X, escape=True):
        '''
        Builds patsy formula string with escaped values e.g. y ~ Q("x1") + Q("x2").
        '''
        vars = X
        if escape:
            vars = [f'Q("{x}")' for x in X]
        return f'{y} ~ {" + ".join(vars)}'

    def univariate_lr(self, df, y):
        '''
        Runs univariate logistic regression with encoded Dataframe as input.
        '''
        variables = sorted({
            col.split(self.variables_seperator)[0] for col in df.columns
            if self.variables_seperator in col
        })

        logit_map = [
            (
                var,
                self.build_formula_string(
                    y, [col for col in df.columns if col.startswith(var)]
                ),
                self.build_formula_string(
                    y, [var], escape=False
                )
            )
            for var in variables
        ]

        return self.run_logit(df, logit_map)

    def multivariate_lr(self, df, y, extra_drop_fields: list = []):
        '''
        Excludes multicollinear variables and runs multivariate logistic regression with encoded Dataframe as input.
        '''

        # DEFINE VARIABLES TO REMOVE LIKE FOR EXAMPLE MULTICOLLINEAR VARIABLES

        drop_fields = [
            # NOTE: These could be exluded, but we prespecified the usage of all variables
            # 'studied_medical_conditions',  # NOTE: High LLR p-value (>0.25)
            # 'has_outcomes',  # NOTE: High LLR p-value (>0.25)

            # NOTE: Correlates with 'data_collection_days_difference' and 'final_report_days_difference'
            'registration_year_grouped',

            # NOTE: High Correlation with updated state, probably filled by ENCEPP team while migrating finalized studies
            'study_topic_grouped'
        ]

        if extra_drop_fields:
            drop_fields += extra_drop_fields

        variables = df.columns[
            ~df.columns.str.split(self.variables_seperator).str[0].isin(
                [y, *drop_fields]
            )
        ].sort_values()

        logit_map = [(
            'all',
            self.build_formula_string(
                y,
                variables
            ),
            self.build_formula_string(
                y,
                variables.str.split(self.variables_seperator).str[0].unique(),
                escape=False
            )
        )]

        return self.run_logit(df, logit_map)

    def run(self, args, opts):
        '''
        Runs statistics script based on analysis plan published on OSF.

        Note that the website was migrated and values / names have changed since February 2024.
        '''
        super().run(args, opts)

        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        from statsmodels.iolib.table import SimpleTable
        from statsmodels.stats.proportion import proportion_confint

        sns.set_theme(context="paper", style="whitegrid")
        (self.output_folder / 'plots/').mkdir(parents=True, exist_ok=True)

        self.downtime = np.arange(
            self.downtime_start, self.downtime_end, dtype='datetime64[D]'
        )

        self.logger = logging.getLogger()
        self.logger.info('Starting statistic script')
        self.logger.info(f'Pandas {self.pd.__version__}')
        self.logger.info('Reading input data...')
        data = self.preprocess(self.read_input())

        # Adding outcomes
        if 'has_protocol' not in data.columns:
            data = data.assign(
                # NOTE: Not used in final analysis. This variable will be assigned beforehand based on the method below with some manually changed classifications
                has_protocol=data.filter(like='protocol').notna()
            )

        if 'has_result' not in data.columns:
            data = data.assign(
                # NOTE: Not used in final analysis. This variable will be assigned beforehand based on manual classification
                has_result=data.filter(
                    like='result'
                ).notna().any(axis='columns')
            )

        self.logger.info('Generating categories...')
        variables = self.create_variables(data)

        self.logger.info('Writing some preanalysis data...')
        self.write_output(data, '_statistics_preprocessed')

        # NOTE: This is the population of studies, which should have protocols available
        variables_due_protocol = variables[variables['due_protocol']]

        # NOTE: This is the population of studies, which should have results available
        variables_due_result = variables[variables['due_result']]

        with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_variables.xlsx', engine='openpyxl') as writer:

            for df, sheet_name in [
                (variables, 'all'),
                (variables_due_protocol, 'due_protocol'),
                (variables_due_result, 'due_result')
            ]:

                df.to_excel(
                    writer,
                    sheet_name=sheet_name
                )

        self.logger.info('Generating and writing part 1 of analysis...')
        for df, suffix in [
            (variables, '_all'),
            (variables_due_protocol, '_due_protocol'),
            (variables_due_result, '_due_result')
        ]:

            with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_variables_frequencies{suffix}.xlsx', engine='openpyxl') as writer:

                # Variables
                df.to_excel(
                    writer,
                    sheet_name=f'variables{suffix}'[
                        :self.max_sheet_name_length]
                )

                # Description of all numerical fields
                # min max mean var etc.
                df.describe().to_excel(
                    writer,
                    sheet_name='numerical_descriptions'
                )

                for col in sorted(df.columns):

                    def calculate_and_write_frequencies(df, col, writer, dropna=True, col_offset=0):

                        # Absolute and relative frequencies of categories
                        frequencies = self.pd.DataFrame().assign(
                            absolute=df.loc[:, [col]].apply(
                                lambda x: x.value_counts(dropna=dropna)),
                            percentage=df.loc[:, [col]].apply(
                                lambda x: x.value_counts(dropna=dropna, normalize=True) * 100)
                        ).reset_index().rename(
                            columns={
                                col: col
                            }
                        )

                        frequencies.to_excel(
                            writer,
                            sheet_name=f'{col}_frequencies'[
                                :self.max_sheet_name_length],
                            index=False,
                            startcol=col_offset
                        )

                        if col in self.category_array_fields:

                            grouped_frequencies = frequencies \
                                .assign(split=lambda x: x[col].str.split('; ')) \
                                .explode('split') \
                                .groupby('split')

                            # Absolute and relative frequencies of subcategories
                            overall_frequencies = self.pd.DataFrame().assign(
                                overall_absolute=grouped_frequencies['absolute'].sum(
                                ),
                                overall_percentage=grouped_frequencies['percentage'].sum(
                                ),
                            ).reset_index().rename(columns={'split': col})

                            overall_frequencies.to_excel(
                                writer,
                                sheet_name=f'{col}_frequencies'[
                                    :self.max_sheet_name_length],
                                index=False,
                                startcol=4 + col_offset
                            )

                    # Frequencies of categories without NA
                    calculate_and_write_frequencies(df, col, writer)
                    if df.loc[:, [col]].isna().any().loc[col]:
                        # Frequencies of categories with NA
                        calculate_and_write_frequencies(
                            df, col, writer, dropna=False,
                            col_offset=8 if col in self.category_array_fields else 4
                        )

        self.logger.info('Generating and writing part 2 of analysis...')
        for df, suffix in [
            (variables, '_all'),
            (variables_due_protocol, '_due_protocol'),
            (variables_due_result, '_due_result')
        ]:

            with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_variables_documents{suffix}.xlsx', engine='openpyxl') as writer:

                df.to_excel(
                    writer,
                    sheet_name=f'variables{suffix}'[
                        :self.max_sheet_name_length]
                )

                for col in ['has_protocol', 'has_result']:

                    def get_frequencies_with_ci(df, alpha=0.05):
                        return df.apply(lambda x: x.value_counts()) \
                            .rename(columns={col: 'absolute'}) \
                            .assign(
                                percentage=df.apply(
                                    lambda x: x.value_counts(normalize=True) * 100),
                                confidence_interval=lambda x: x['absolute'].apply(
                                    lambda y: [z * 100 for z in proportion_confint(y, len(df), alpha=alpha, method='beta')])
                        ).reset_index()

                    # Absolute and relative frequencies (with 95%-CI) of categories with protocols or results
                    frequencies = get_frequencies_with_ci(df.loc[:, [col]])

                    frequencies.to_excel(
                        writer,
                        sheet_name=col[:self.max_sheet_name_length],
                        index=False
                    )

                    # Same metrics for the subset of studies required by RMP
                    required_rmp_frequencies = get_frequencies_with_ci(
                        df.loc[df['risk_management_plan'].isin(
                            self.required_rmp), [col]]
                    ).rename(columns={col: f'required_{col}'})

                    required_rmp_frequencies.to_excel(
                        writer,
                        sheet_name=col[:self.max_sheet_name_length],
                        index=False,
                        startrow=4
                    )

        self.logger.info('Generating and writing part 3 of analysis...')
        data_to_group = variables.merge(
            data.loc[:, [self.group_by_field_name, 'countries']],
            left_index=True,
            right_index=True
        )
        grouped_agg = self.create_grouped_agg(data_to_group)
        self.write_output(grouped_agg, '_statistics_funding_all')

        self.logger.info('Write website data...')
        grouped_agg.reset_index().rename(
            columns={self.group_by_field_name: 'name'}
        ).to_json(
            self.output_folder / 'funding.json',
            orient='records',
            force_ascii=False
        )

        data.loc[:, [
            'title', 'registration_date', '$UPDATED_state', 'url'
        ]].rename(
            columns={'$UPDATED_state': 'state'}
        ).to_json(
            self.output_folder / 'studies.json',
            orient='index',
            force_ascii=False
        )

        logit_data = []
        for df, y_label, name in [
            (variables_due_protocol, 'has_protocol', 'protocol'),
            (variables_due_result, 'has_result', 'results')
        ]:

            self.logger.info(
                f'Starting logistic regression for {y_label}...')

            self.logger.info(
                'Generating and writing encoded variables for logistic regression...')
            encoded = self.encode_variables(df)
            y = df.loc[:, [y_label]].astype(int)
            encoded_y = encoded.merge(
                y,
                left_index=True,
                right_index=True,
                how='right'
            )
            self.write_output(
                encoded_y, f'_statistics_encoded_variables_{name}')

            self.logger.info(
                'Generating and writing correlations for logistic regression...')
            correlations = self.encode_variables(
                df,
                drop_references=False
            ).merge(
                y,
                left_index=True,
                right_index=True,
                how='right'
            ).corr(method='pearson')
            self.write_output(
                correlations, f'_statistics_encoded_variables_correlations_for_{name}_model')

            fig, ax = plt.subplots(figsize=(20, 15), dpi=300)
            ax.set_title('Correlations (Pearson)')
            mask = np.triu(np.ones_like(correlations, dtype=bool))
            sns.heatmap(correlations, mask=mask, cmap='RdBu', vmax=.3, center=0,
                        square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
            fig.savefig(self.output_folder / 'plots' /
                        f'correlation_heatmap_for_{name}_model.png', bbox_inches='tight')

            def save_model_results(results, folder_name, subfolder_name):
                (self.output_folder / folder_name / 'models' /
                 subfolder_name).mkdir(parents=True, exist_ok=True)
                (self.output_folder / folder_name / 'summaries' /
                 subfolder_name).mkdir(parents=True, exist_ok=True)

                summaries = {}

                for name, model_result in results.items():
                    model_result.save(
                        self.output_folder / folder_name / 'models' / subfolder_name / f'{name}.pickle')

                    ci_odds_ratio = np.exp(model_result.conf_int()) \
                        .rename(columns={0: '[0.025', 1: '0.975]'})
                    odds_ratio = np.exp(model_result.params) \
                        .rename('odds rt').to_frame()
                    odds_ratio_data = np.round(
                        self.pd.merge(
                            odds_ratio,
                            ci_odds_ratio,
                            left_index=True,
                            right_index=True
                        ),
                        decimals=4
                    )

                    table = SimpleTable(
                        odds_ratio_data.values, odds_ratio_data.columns.to_list()
                    )
                    summary = model_result.summary()
                    summary.tables[1].extend_right(table)

                    summary_df = self.pd.DataFrame(
                        summary.tables[1].data[1:],
                        columns=['name'] + summary.tables[1].data[0][1:]
                    )
                    summary_df = summary_df.set_index(summary_df.columns[0])
                    summaries.setdefault(
                        name,
                        summary_df
                    )

                    (self.output_folder / folder_name /
                     'summaries' / subfolder_name / f'{name}.txt') \
                        .write_text(summary.as_text())

                    (self.output_folder / folder_name /
                     'summaries' / subfolder_name / f'{name}.html') \
                        .write_text(summary.as_html())

                    (self.output_folder / folder_name /
                     'summaries' / subfolder_name / f'{name}.csv') \
                        .write_text(summary.as_csv())

                return summaries

            self.logger.info(
                'Running univariate logistic regression and writing output...')
            results = self.univariate_lr(encoded_y, y_label)
            univariate_summaries = save_model_results(
                results,
                'univariate_models',
                name
            )

            self.logger.info(
                'Running multivariate logistic regression and writing output...')
            results = self.multivariate_lr(
                encoded_y, y_label,
                extra_drop_fields=[
                    'final_report_days_difference'
                    if name == 'protocol'
                    else 'data_collection_days_difference'
                ]
            )
            multivariate_summaries = save_model_results(
                results,
                'multivariate_models',
                name
            )

            logit_data.append(
                dict(univariate_summaries, **multivariate_summaries)
            )

        self.logger.info('Generating and writing extra tables...')
        with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_tables_frequencies.xlsx', engine='openpyxl') as writer:

            for df, suffix in [
                (variables, '_all'),
                (variables_due_protocol, '_due_protocol'),
                (variables_due_result, '_due_result')
            ]:

                # df.to_excel(
                #     writer,
                #     sheet_name=f'variables{suffix}'[
                #         :self.max_sheet_name_length]
                # )

                # Create Frequency table of variables for each rmp category
                rmp = df['risk_management_plan'].fillna('Unspecified')
                result = self.pd.DataFrame()

                for col in self.frequency_fields:

                    # Absolute Frequencies
                    absolute = self.pd.crosstab(
                        df[col].fillna('Not available'),
                        rmp,
                        rownames=['value'],
                        margins=True,
                        margins_name='All',
                    )

                    # Absolute Frequencies (This will drop the 'All' row automatically)
                    relative = self.pd.crosstab(
                        df[col].fillna('Not available'),
                        rmp,
                        rownames=['value'],
                        margins=True,
                        margins_name='All',
                        normalize='columns'
                    )

                    combined = absolute[:-1].astype(str) + ' (' + (
                        relative * 100).round(1).astype(str) + ')'

                    # NOTE: Quickfix to prevent dtype changes for planned_duration_quartiles
                    # NOTE: This will also change the bool value names
                    combined.index = combined.index.astype(str)

                    result = self.pd.concat([
                        result,
                        combined.assign(variable=col)
                    ])

                result = result \
                    .reset_index() \
                    .set_index(['variable', 'value'])

                # Cleanup column names and sort them
                result.columns = result.columns.str.replace(
                    r'\s\(.+\)', '', regex=True) + ' (' + absolute.iloc[-1].astype(str).values + ')'

                result = result[[
                    *result.columns[-1:].values, *result.columns[:-1].values
                ]]

                result.to_excel(
                    writer,
                    sheet_name=f'table{suffix}'[
                        :self.max_sheet_name_length]
                )

        with self.pd.ExcelWriter(self.output_folder / f'{self.input_path.stem}_statistics_tables_logit.xlsx', engine='openpyxl') as writer:

            for df, y_label, suffix, logit in zip(
                (variables_due_protocol, variables_due_result),
                ('has_protocol', 'has_result'),
                ('_due_protocol', '_due_result'),
                logit_data
            ):

                def transform_logit_table(df):
                    df = df.drop('Intercept').reset_index()

                    df['name'] = df['name'].str.replace(
                        self.formula_formatter_regex, r'\1',
                        regex=True
                    )

                    df = df.assign(
                        variable=df['name'].str.split(
                            self.variables_seperator).str[0],
                        value=df['name'].str.split(
                            self.variables_seperator).str[1]
                    )

                    df = self.pd.concat([
                        df.iloc[:, :5],
                        df.iloc[:, 7:]
                    ],
                        axis='columns'
                    )

                    df['odds rt'] = df['odds rt'].round(2).map('{:,.2f}'.format) + ' (' + \
                        df['[0.025'].round(2).map('{:,.2f}'.format) + ' - ' + \
                        df['0.975]'].round(2).map('{:,.2f}'.format) + ')'

                    df = df.drop(
                        [
                            'coef', 'std err',
                            'z', '[0.025', '0.975]'
                        ],
                        axis='columns'
                    )

                    df = df.rename(columns={
                        'P>|z|': 'P value',
                        'odds rt': 'OR (95% CI)'
                    })

                    # Reorder columns for better readability
                    return df[df.columns[::-1]]

                # df.to_excel(
                #     writer,
                #     sheet_name=f'variables{suffix}'[
                #         :self.max_sheet_name_length]
                # )

                multivariate = transform_logit_table(logit['all'])

                univariate = self.pd.DataFrame()
                frequencies = self.pd.DataFrame()
                for var, logit_df in logit.items():
                    if var != 'all':

                        univariate = self.pd.concat([
                            univariate,
                            transform_logit_table(logit_df)
                        ])

                        # NOTE: The NA Values have to be filled with 'nan' to merge values correctly
                        # We will rename this later
                        frequencies = self.pd.concat([
                            frequencies,
                            self.pd.crosstab(
                                df[var].fillna(str(np.nan)),
                                df[y_label],
                                rownames=['value'],
                                margins=True,
                                margins_name='N'
                            )
                            .drop(columns=[False])
                            .drop(index=['N'])
                            .assign(
                                variable=var,
                                relative=lambda x: x[True] / x['N']
                            )
                            .reset_index()
                            .rename(
                                columns={
                                    True: 'absolute'
                                }
                            )
                        ])

                frequencies = frequencies.assign(
                    **{
                        'n (%)': lambda x: x['absolute'].astype(str) + ' ('
                        + (x['relative'] * 100).round(1).astype(str) + ')'
                    },
                    name=lambda x: x['variable'].astype(str) + '__'
                    + x['value'].astype(str).map(self.python_name_converter)
                ).drop(['absolute', 'relative'], axis='columns')

                result = self.pd.merge(
                    univariate,
                    multivariate,
                    on='name',
                    how='left',
                    suffixes=(' univariate', ' multivariate')
                )

                result = self.pd.merge(
                    frequencies,
                    result,
                    on='name',
                    how='left'
                ).drop(
                    ['name', 'variable univariate', 'value univariate',
                     'variable multivariate', 'value multivariate'],
                    axis='columns'
                )

                # NOTE: Quickfix to prevent dtype changes for planned_duration_quartiles
                # NOTE: This will also change the bool value names
                result['value'] = result['value'].astype(str).map(
                    lambda x: 'Not available' if x == 'nan' else x
                )

                result = result.set_index(['variable', 'value']).sort_index()

                result.to_excel(
                    writer,
                    sheet_name=f'table{suffix}'[
                        :self.max_sheet_name_length]
                )

        self.logger.info('Generating and writing extra plots...')

        plt.figure(dpi=300)
        for df, label, name in [
            (data, 'all studies', 'all'),
            (variables_due_protocol, 'studies with protocol due', 'due_protocol'),
            (variables_due_result, 'studies with results due', 'due_results')
        ]:
            date = df['registration_date'].dt.to_period('M')
            self.pd.concat(
                [
                    df.groupby(date).size().rename('studies'),
                    df.groupby(date).size().cumsum().rename(
                        'cumulated studies')
                ], axis='columns') \
                .plot(
                    title=f'Frequency of {label} by "Registration Date"',
                    xlabel='Registration Date',
                    ylabel='# of studies',
                    subplots=True
            )
            plt.savefig(self.output_folder / 'plots' /
                        f'registration_date_count_freq_{name}.png')

        plt.figure(dpi=300)
        variables.groupby(date)[['has_protocol', 'has_result']].sum().plot(
            title='Frequency of studies with protocol or results by "Registration Date"',
            xlabel='Registration Date',
            ylabel='# of studies',
            subplots=True
        )
        plt.savefig(self.output_folder / 'plots' /
                    'registration_date_protocol_results_freq.png')
